{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoteachingIMDb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+e2QrIc+FvUwyqmrnR9bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leon0702/Co-teachingIMDb/blob/main/CoteachingIMDb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz-g2BUSJBGQ",
        "outputId": "a2e515ba-501a-4874-df22-93f9f4e08e12"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential\r\n",
        "from keras import layers\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras import backend as K\r\n",
        "import sys\r\n",
        "import time\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL6cevWaJB9g"
      },
      "source": [
        "def bce(x,y):\r\n",
        "  b = tf.keras.losses.BinaryCrossentropy(from_logits = False, reduction = tf.keras.losses.Reduction.NONE)\r\n",
        "  loss = b(y,x)\r\n",
        "  return loss\r\n",
        "def colossA(x1, y1, x2, y2, BS, FR):\r\n",
        "  loss_1 = bce(y1,x1)\r\n",
        "  loss_1_sortedtemp, loss_1_ind = tf.nn.top_k(-loss_1, len(x2))\r\n",
        "  loss_1_sorted = loss_1_sortedtemp*-1\r\n",
        "  loss_2 = bce(y2,x2)\r\n",
        "  loss_2_sortedtemp, loss_2_ind = tf.nn.top_k(-loss_2, len(x2))\r\n",
        "  loss_2_sorted = loss_2_sortedtemp*-1\r\n",
        "  remember_rate = 1 - FR\r\n",
        "  newBS = int(len(x2)*remember_rate)\r\n",
        "  a_data = []\r\n",
        "  a_dataind = []\r\n",
        "  b_data = []\r\n",
        "  a_label = []\r\n",
        "  b_label = []\r\n",
        "  for i in range(0,newBS):\r\n",
        "    a_data.append(x2[loss_2_ind[i]])\r\n",
        "    b_data.append(x1[loss_1_ind[i]])\r\n",
        "    a_dataind.append(loss_2_ind[i])\r\n",
        "    a_label.append(y2[loss_2_ind[i]])\r\n",
        "    b_label.append(y1[loss_1_ind[i]])\r\n",
        "  a_data = np.array(a_data)\r\n",
        "  b_data = np.array(b_data)\r\n",
        "  a_dataind = np.array(a_dataind)\r\n",
        "  a_label = np.array(a_label)\r\n",
        "  return  a_dataind, a_label\r\n",
        "def colossB(x1, y1, x2, y2, BS, FR):\r\n",
        "  loss_1 = bce(y1,x1)\r\n",
        "  loss_1_sortedtemp, loss_1_ind = tf.nn.top_k(-loss_1, len(x1))\r\n",
        "  loss_1_sorted = loss_1_sortedtemp*-1\r\n",
        "  loss_2 = bce(y2,x2)\r\n",
        "  loss_2_sortedtemp, loss_2_ind = tf.nn.top_k(-loss_2, len(x1))\r\n",
        "  loss_2_sorted = loss_2_sortedtemp*-1\r\n",
        "  remember_rate = 1 - FR\r\n",
        "  newBS = int(len(x1)*remember_rate)\r\n",
        "  a_data = []\r\n",
        "  b_dataind = []\r\n",
        "  b_data = []\r\n",
        "  a_label = []\r\n",
        "  b_label = []\r\n",
        "  for i in range(0,newBS):\r\n",
        "    a_data.append(x2[loss_2_ind[i]])\r\n",
        "    b_data.append(x1[loss_1_ind[i]])\r\n",
        "    b_dataind.append(loss_1_ind[i])\r\n",
        "    a_label.append(y2[loss_2_ind[i]])\r\n",
        "    b_label.append(y1[loss_1_ind[i]])\r\n",
        "  a_data = np.array(a_data)\r\n",
        "  b_data = np.array(b_data)\r\n",
        "  b_dataind = np.array(b_dataind)\r\n",
        "  a_label = np.array(a_label)\r\n",
        "  b_label = np.array(b_label)\r\n",
        "  \"\"\"\r\n",
        "  print(\"colossB\")\r\n",
        "  print(\"FR =\",FR)\r\n",
        "  print(\"Remember_rate =\",remember_rate)\r\n",
        "  print(\"b_dataind =\",b_dataind)\r\n",
        "  \"\"\"\r\n",
        "  return  b_dataind, b_label\r\n",
        "def trainA(dataa,labela,labelb,BS,EP,logitsa,logitsb):\r\n",
        "  \r\n",
        "  newAdata_ind, newAlabels = colossA(logitsa,labela,logitsb,labelb,BS,rate_schedule[EP])\r\n",
        "  #print(\"newAdata_ind\",newAdata_ind)\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    logits = Onea(dataa[newAdata_ind],training = True)\r\n",
        "    loss_value = bce(logits, newAlabels)\r\n",
        "\r\n",
        "  trainable_varsA = Onea.trainable_variables\r\n",
        "  gradients = tape.gradient(loss_value, trainable_varsA)\r\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_varsA))\r\n",
        "\r\n",
        "  A_loss(loss_value)\r\n",
        "  for metric in A_accuracy:\r\n",
        "    metric(newAlabels, logits)\r\n",
        "  print_status_bar(i*BS,len(One_partial_x_train),A_loss,A_accuracy)\r\n",
        "  return  A_loss.result(), metric.result()\r\n",
        "def trainB(labela,datab,labelb,BS,EP,logitsa,logitsb):\r\n",
        "  newBdata_ind, newBlabels = colossB(logitsa,labela,logitsb,labelb,BS,rate_schedule[EP])\r\n",
        "  #print(\"newBdata_ind\",newBdata_ind)\r\n",
        "  with tf.GradientTape() as t:\r\n",
        "    logits = Oneb(datab[newBdata_ind],training = True)\r\n",
        "    loss_value = bce(logits, newBlabels)\r\n",
        "\r\n",
        "  trainable_varsB = Oneb.trainable_variables\r\n",
        "  gradients = t.gradient(loss_value, trainable_varsB)\r\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_varsB))\r\n",
        "\r\n",
        "  B_loss(loss_value)\r\n",
        "  for metric in B_accuracy:\r\n",
        "    metric(newBlabels, logits)\r\n",
        "  print_status_bar(i*BS,len(One_partial_x_train),B_loss,B_accuracy)\r\n",
        "  return  B_loss.result(), metric.result()\r\n",
        "def valA(data,label):\r\n",
        "  logita = Onea(data, training = False)\r\n",
        "  loss_value = bce(logita, label)\r\n",
        "  A_val.update_state(label, logita)\r\n",
        "  A_val_loss.update_state(loss_value)\r\n",
        "  return A_val.result(), A_val_loss.result()\r\n",
        "def valB(data,label):\r\n",
        "  logitb = Oneb(data, training = False)\r\n",
        "  loss_value = bce(logitb, label)\r\n",
        "  B_val.update_state(label, logitb)\r\n",
        "  B_val_loss.update_state(loss_value)\r\n",
        "  return B_val.result(), B_val_loss.result()\r\n",
        "def progress_bar(iteration, total, size=30):\r\n",
        "  running = iteration < total\r\n",
        "  c = \">\" if running else \"=\"\r\n",
        "  p = (size - 1) * iteration // total\r\n",
        "  fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\r\n",
        "  params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\r\n",
        "  return fmt.format(*params)\r\n",
        "def print_status_bar(iteration, total, loss, metrics=None, size=30):\r\n",
        "  metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\r\n",
        "                      for m in [loss] + (metrics or [])])\r\n",
        "  end = \"\" if iteration < total else \"\\n\"\r\n",
        "  print(\"\\r{} - {}\".format(progress_bar(iteration, total),\"\"\" metrics\"\"\"), end=end)\r\n",
        "def computeEnsemble(xnow,EP,templogit):\r\n",
        "    #templogit = templogit*(1-(alpha*(EP/EPOCH)))+xnow*(alpha*(EP/EPOCH))\r\n",
        "    templogit = (templogit*(1-alpha)+xnow*alpha)\r\n",
        "    return templogit\r\n",
        "def find_noise(y,ensx,tempx):\r\n",
        "  a_dataind = []\r\n",
        "  a_label = []\r\n",
        "  tempind = []\r\n",
        "  noisecount = 0\r\n",
        "  cleancount = 0\r\n",
        "  tempclean = 0\r\n",
        "  tempnoise = 0\r\n",
        "  #print(\"Ensemble accuracy = \",tf.keras.metrics.BinaryAccuracy()(y,ensx))\r\n",
        "  #print(\"Start Check and Count\")\r\n",
        "  for i in range(0,len(ensx)):\r\n",
        "    #觀察ensemble的預測結果\r\n",
        "    if ensx[i]>=0.5:\r\n",
        "        prelab = 1.0\r\n",
        "    else:\r\n",
        "        prelab = 0.0\r\n",
        "    if prelab == y[i]:\r\n",
        "        cleancount +=1\r\n",
        "        #a_dataind.append(i)\r\n",
        "        a_label.append(y[i])\r\n",
        "    else:\r\n",
        "        noisecount +=1\r\n",
        "        \r\n",
        "        a_dataind.append(i)\r\n",
        "        \"\"\"\r\n",
        "        if y[i] == 1.0:\r\n",
        "            a_label.append(1.0-y[i])\r\n",
        "        else:\r\n",
        "            a_label.append(1.0+y[i])\r\n",
        "        \"\"\"\r\n",
        "        #print(\"Maybe Noise Number\",i,\" = \",ensx[i])\r\n",
        "    #觀察目前為止的預測結果\r\n",
        "    if tempx[i]>=0.5:\r\n",
        "        templab = 1.0\r\n",
        "    else:\r\n",
        "        templab = 0.0\r\n",
        "    if templab == y[i]:\r\n",
        "        tempclean +=1\r\n",
        "    else:\r\n",
        "        tempnoise +=1\r\n",
        "        tempind.append(i)\r\n",
        "  a_dataind = np.array(a_dataind)\r\n",
        "  tempind = np.array(tempind)\r\n",
        "  a_label = np.array(a_label)\r\n",
        "  print(\"\\nEnsemble Noise number = \",noisecount)\r\n",
        "  print(\"Ensemble Clean number = \",cleancount)\r\n",
        "  print(\"Now Noise number = \",tempnoise)\r\n",
        "  print(\"Now Clean number = \",tempclean)\r\n",
        "  return  a_dataind, a_label,tempind\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CE-EC-CJM1A",
        "outputId": "abeb5b47-ddd6-43f5-df52-a7e0f889551e"
      },
      "source": [
        "#-----0% to 40%\r\n",
        "#--------------------------------------讀訓練資料\r\n",
        "data_dir = '/content/drive/My Drive/Colab Notebooks/IMDb/0%noise'\r\n",
        "train_dir = os.path.join(data_dir, 'train_1.txt')\r\n",
        "\r\n",
        "f= open(train_dir, encoding='UTF-8')\r\n",
        "train_data = f.read()\r\n",
        "f.close()\r\n",
        "\r\n",
        "train_data = train_data.split('\\n')\r\n",
        "train_data= train_data[:-1]\r\n",
        "\r\n",
        "labels = []\r\n",
        "train_texts = []\r\n",
        "ind = []\r\n",
        "for index, item in enumerate(train_data):\r\n",
        "    ind.append(index)\r\n",
        "    label=[x for x in item.split(',')]\r\n",
        "    text=label[3:]\r\n",
        "    text=\" \".join(text)\r\n",
        "    train_texts.append(text)\r\n",
        "    if label[2]=='1':\r\n",
        "        labels.append(1)\r\n",
        "    else:\r\n",
        "        labels.append(0)\r\n",
        "#-------------------------------------------------------------------------------------------------------------------讀順序\r\n",
        "data_dir = '/content/drive/My Drive/Colab Notebooks/IMDb/0%noise'\r\n",
        "train_dir = os.path.join(data_dir, 'A_train.txt')\r\n",
        "\r\n",
        "f= open(train_dir, encoding='UTF-8')\r\n",
        "train = f.read()\r\n",
        "f.close()\r\n",
        "\r\n",
        "train = train.split('\\n')\r\n",
        "train= train[:-1]\r\n",
        "#--------------------------------------讀測試訓練資料\r\n",
        "data_dir = '/content/drive/My Drive/Colab Notebooks/IMDb/0%noise'\r\n",
        "fname = os.path.join(data_dir, 'test_1.txt')\r\n",
        "\r\n",
        "f= open(fname, encoding='UTF-8')\r\n",
        "test = f.read()\r\n",
        "f.close()\r\n",
        "\r\n",
        "test = test.split('\\n')\r\n",
        "test= test[:-1]\r\n",
        "\r\n",
        "y_labels=[]\r\n",
        "test_texts=[]\r\n",
        "\r\n",
        "for index, item in enumerate(test):\r\n",
        "    label=[x for x in item.split(',')]\r\n",
        "    text=label[3:]\r\n",
        "    text=\" \".join(text)\r\n",
        "    test_texts.append(text)\r\n",
        "    if label[2]=='1':\r\n",
        "        y_labels.append(1)\r\n",
        "    else:\r\n",
        "        y_labels.append(0)\r\n",
        "\r\n",
        "#--------------------------------------------轉訓練資料順序 要先轉順序才可以加noise\r\n",
        "\r\n",
        "train_texts_1=[]\r\n",
        "labels_1 =[]\r\n",
        "for indexs in range(len(train)):\r\n",
        "    number = int(train[indexs])\r\n",
        "    train_texts_1.append(train_texts[number])\r\n",
        "    labels_1.append(labels[number])\r\n",
        "train_texts = train_texts_1\r\n",
        "labels = labels_1\r\n",
        "clean_labels = labels.copy()\r\n",
        "texts = train_texts+test_texts\r\n",
        "\r\n",
        "#------------------------------------------------------------------------------------------------------------讀noise\r\n",
        "data_dir = '/content/drive/My Drive/Colab Notebooks/IMDb/40%noise'\r\n",
        "noise_dir = os.path.join(data_dir, 'A_noise.txt')\r\n",
        "\r\n",
        "f= open(noise_dir, encoding='UTF-8')\r\n",
        "noise = f.read()\r\n",
        "f.close()\r\n",
        "\r\n",
        "noise = noise.split('\\n')\r\n",
        "noise= noise[:-1]\r\n",
        "co = []\r\n",
        "valco = []\r\n",
        "#------------------------------------------------------------加入noise\r\n",
        "\r\n",
        "for indexes in range(len(noise)):\r\n",
        "    number = int(noise[indexes])\r\n",
        "    if  number >=2000:\r\n",
        "        co.append(number)\r\n",
        "        if labels[number]>=1:\r\n",
        "            labels[number]=0\r\n",
        "        else:\r\n",
        "            labels[number]=1\r\n",
        "    elif number<2000:\r\n",
        "        valco.append(number)\r\n",
        "\r\n",
        "print(len(co),len(valco))\r\n",
        "\r\n",
        "#-------------------------------------------------前處理Tokenzier\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import numpy as np\r\n",
        "maxlen = 1000  # We will cut reviews after 1200 words\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=10000, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',oov_token = 'unk')\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "sequences = tokenizer.texts_to_sequences(texts)\r\n",
        "\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "data = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\r\n",
        "#---------------------------------------------分train, test data\r\n",
        "x_train=data[:25000]\r\n",
        "x_test=data[25000:]\r\n",
        "\r\n",
        "y_train = np.zeros((len(labels),1))\r\n",
        "y_train = np.asarray(y_train).astype('float32')\r\n",
        "y_test = np.zeros((len(y_labels),1))\r\n",
        "y_test = np.asarray(y_test).astype('float32')\r\n",
        "#----------------------------------------------label\r\n",
        "for i in range(0,(len(labels))):\r\n",
        "  if labels[i] == 1.0:\r\n",
        "    y_train[i] = 1.0\r\n",
        "for i in range(0,len(y_labels)):\r\n",
        "  if y_labels[i] == 1.0:\r\n",
        "    y_test[i] = 1.0\r\n",
        "#----------------------------------------------------切資料\r\n",
        "x_val = x_train[:2000]\r\n",
        "partial_x_train = x_train[2000:]\r\n",
        "#partial_x_train = np.append(pxt,pxt2,axis = 0)\r\n",
        "\r\n",
        "y_val = y_train[:2000]\r\n",
        "partial_y_train = y_train[2000:]\r\n",
        "#partial_y_train = np.append(pyt,pyt2)\r\n",
        "max_features=10000\r\n",
        "#-------------------------------------------------------One hot\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def vectorize_sequences(sequences, dimension=10000):\r\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\r\n",
        "    results = np.zeros((len(sequences), dimension))\r\n",
        "    for i, sequence in enumerate(sequences):\r\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\r\n",
        "    return results\r\n",
        "\r\n",
        "One_x_train = vectorize_sequences(x_train)# 整筆訓練資料\r\n",
        "One_partial_x_train = vectorize_sequences(partial_x_train) #One hot \r\n",
        "One_x_val = vectorize_sequences(x_val)# One-hot 輸入的驗證資料\r\n",
        "One_x_test = vectorize_sequences(x_test)# One-hot 輸入的測試資料"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9198 802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRfhnz30JgeO",
        "outputId": "9a480aa6-4a8d-42cf-8afb-93044531d620"
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\r\n",
        "A_loss = tf.keras.metrics.Mean()\r\n",
        "B_loss = tf.keras.metrics.Mean()\r\n",
        "A_accuracy = [tf.keras.metrics.BinaryAccuracy(threshold=0.5)]\r\n",
        "B_accuracy = [tf.keras.metrics.BinaryAccuracy(threshold=0.5)]\r\n",
        "A_val_loss = tf.keras.metrics.Mean()\r\n",
        "B_val_loss = tf.keras.metrics.Mean()\r\n",
        "A_val = tf.keras.metrics.BinaryAccuracy()\r\n",
        "B_val = tf.keras.metrics.BinaryAccuracy()\r\n",
        "# Keep results for plotting\r\n",
        "train_loss_resultsA = []\r\n",
        "train_accuracy_resultsA = []\r\n",
        "train_loss_resultsB = []\r\n",
        "train_accuracy_resultsB = []\r\n",
        "val_loss_resultsA = []\r\n",
        "val_accuracy_resultsA = []\r\n",
        "val_loss_resultsB = []\r\n",
        "val_accuracy_resultsB = []\r\n",
        "EPOCH = 30\r\n",
        "BS = 512\r\n",
        "forget_rate = 0.4\r\n",
        "alpha = 1.0\r\n",
        "exponent = 1 #exponent of the forget rate. This parameter is equal to c in Tc for R(T) in Co-teaching paper.\r\n",
        "num_gradual = EPOCH+1 #how many epochs for linear drop rate. This parameter is equal to Tk for R(T) in Co-teaching paper.\r\n",
        "rate_schedule = np.ones(EPOCH+1)*forget_rate\r\n",
        "rate_schedule[:num_gradual] = np.linspace(0, forget_rate**exponent, num_gradual)\r\n",
        "\r\n",
        "#-----------------------One Hot\r\n",
        "dataA = One_partial_x_train\r\n",
        "dataB = One_partial_x_train\r\n",
        "labelsA = partial_y_train\r\n",
        "labelsB = partial_y_train\r\n",
        "val_dataA = One_x_val\r\n",
        "val_dataB = One_x_val\r\n",
        "val_labelsA = y_val\r\n",
        "val_labelsB = y_val\r\n",
        "\r\n",
        "pre_numupdates = int(len(One_partial_x_train)/BS)\r\n",
        "numupdates = int(len(One_partial_x_train)/BS)\r\n",
        "val_numupdates = int(len(One_x_val)/BS)\r\n",
        "print(rate_schedule)\r\n",
        "Onea = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Dense(16, activation = tf.nn.leaky_relu, input_shape = (10000,)),\r\n",
        "    #tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.Dense(16, activation = tf.nn.leaky_relu),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\r\n",
        "    ],)\r\n",
        "Oneb = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Dense(16, activation = tf.nn.leaky_relu, input_shape = (10000,)),\r\n",
        "    #tf.keras.layers.BatchNormalization(),\r\n",
        "    tf.keras.layers.Dense(16, activation = tf.nn.leaky_relu),\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\r\n",
        "    ],)\r\n",
        "#Onea = tf.keras.models.load_model('Model/OneaBest',compile = False)\r\n",
        "#Oneb = tf.keras.models.load_model('Model/OneaBest',compile = False)\r\n",
        "earlycount = 0\r\n",
        "tempAval = 100\r\n",
        "tempBval = 100\r\n",
        "hitlist = []\r\n",
        "noiselist = []\r\n",
        "nowhitlist = []\r\n",
        "nownoiselist = []\r\n",
        "y_ensemble = np.zeros((len(partial_y_train),1))\r\n",
        "for epoch in range(1,EPOCH+1):\r\n",
        "  \r\n",
        "  if earlycount >1:\r\n",
        "    print(\"Early Stopping!!\")\r\n",
        "    break\r\n",
        "  \r\n",
        "  print(\"[INFO] starting epoch {}/{}...\".format(epoch, EPOCH))\r\n",
        "  sys.stdout.flush()\r\n",
        "  epochStart = time.time()\r\n",
        "  localtime = time.ctime()\r\n",
        "  print(\"Start at\", localtime)\r\n",
        "  \r\n",
        "  for i in range(0,numupdates+1):\r\n",
        "    #print(\"Iteration :\",i+1)\r\n",
        "    start = i * BS\r\n",
        "    end = start + BS\r\n",
        "    logitsa = Onea(dataA[start:end],training = False)\r\n",
        "    logitsb = Oneb(dataB[start:end],training = False)\r\n",
        "    loA, accA = trainA(dataA[start:end],labelsA[start:end],labelsB[start:end],BS,epoch,logitsa,logitsb)\r\n",
        "    loB, accB = trainB(labelsA[start:end],dataB[start:end], labelsB[start:end],BS,epoch,logitsa,logitsb)\r\n",
        "  train_loss_resultsA.append(loA)\r\n",
        "  train_accuracy_resultsA.append(accA)\r\n",
        "  train_loss_resultsB.append(loB)\r\n",
        "  train_accuracy_resultsB.append(accB)\r\n",
        "  for i in range(0,val_numupdates+1):\r\n",
        "    start = i * BS\r\n",
        "    end = start + BS\r\n",
        "    valA_acc, valA_loss = valA(val_dataA[start:end],val_labelsA[start:end])\r\n",
        "    valB_acc, valB_loss = valB(val_dataB[start:end],val_labelsB[start:end])\r\n",
        "  if ((valA_loss>tempAval) and (valB_loss>tempBval)):\r\n",
        "    earlycount = earlycount+1\r\n",
        "  elif ((valA_loss<tempAval) and (valB_loss<tempBval)):\r\n",
        "    earlycount = 0\r\n",
        "  print(\"\\nEarly Stop count\",earlycount)\r\n",
        "  val_loss_resultsA.append(valA_loss)\r\n",
        "  val_accuracy_resultsA.append(valA_acc)\r\n",
        "  val_loss_resultsB.append(valB_loss)\r\n",
        "  val_accuracy_resultsB.append(valB_acc)\r\n",
        "  tempAval = valA_loss\r\n",
        "  tempBval = valB_loss\r\n",
        "  A_val.reset_states()\r\n",
        "  B_val.reset_states()\r\n",
        "  A_val_loss.reset_states()\r\n",
        "  B_val_loss.reset_states()\r\n",
        "  #alpha = max(0.1,(alpha - (alpha/EPOCH)))\r\n",
        "  #print(\"ALPHA\",alpha)\r\n",
        "  print(\"Epoch {:d}: Loss A: {:.3f}, Accuracy A: {:.3%}, Loss B: {:.3f}, Accuracy B: {:.3%}\".format(epoch, loA, accA, loB, accB))\r\n",
        "  print(\"Epoch {:d}: Val_A_Loss: {:.3f}, Val_A_Accuracy: {:.3%}, Val_B_Loss: {:.3f}, Val_B_Accuracy: {:.3%}\".format(epoch, valA_loss, valA_acc, valB_loss, valB_acc))\r\n",
        "  # show timing information for the epoch\r\n",
        "  for metric in [A_loss]+A_accuracy:\r\n",
        "    metric.reset_states()\r\n",
        "  for metric in [B_loss]+B_accuracy:\r\n",
        "    metric.reset_states()\r\n",
        "  \r\n",
        "  epochEnd = time.time()\r\n",
        "  ltime = time.ctime()\r\n",
        "  print(\"End at\", ltime)\r\n",
        "  elapsed = (epochEnd - epochStart)\r\n",
        "  print(\"Used {:.4} seconds\".format(elapsed))\r\n",
        "Onea.compile(\r\n",
        "              loss='binary_crossentropy',\r\n",
        "\t\t\t\t\t\t\tmetrics=[\"acc\"])\r\n",
        "(lossa, acca) = Onea.evaluate(One_x_test, y_test,batch_size=BS)\r\n",
        "print(\"[INFO] test accuracy: {:.4f}\".format(acca))\r\n",
        "Oneb.compile(\r\n",
        "              loss='binary_crossentropy',\r\n",
        "\t\t\t\t\t\t\tmetrics=[\"acc\"])\r\n",
        "(lossb, accb) = Oneb.evaluate(One_x_test, y_test,batch_size=BS)\r\n",
        "print(\"[INFO] test accuracy: {:.4f}\".format(accb))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.01333333 0.02666667 0.04       0.05333333 0.06666667\n",
            " 0.08       0.09333333 0.10666667 0.12       0.13333333 0.14666667\n",
            " 0.16       0.17333333 0.18666667 0.2        0.21333333 0.22666667\n",
            " 0.24       0.25333333 0.26666667 0.28       0.29333333 0.30666667\n",
            " 0.32       0.33333333 0.34666667 0.36       0.37333333 0.38666667\n",
            " 0.4       ]\n",
            "[INFO] starting epoch 1/30...\n",
            "Start at Mon Jan 11 11:46:39 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 1: Loss A: 0.692, Accuracy A: 51.611%, Loss B: 0.692, Accuracy B: 51.532%\n",
            "Epoch 1: Val_A_Loss: 0.684, Val_A_Accuracy: 60.200%, Val_B_Loss: 0.683, Val_B_Accuracy: 60.450%\n",
            "End at Mon Jan 11 11:47:23 2021\n",
            "Used 43.61 seconds\n",
            "[INFO] starting epoch 2/30...\n",
            "Start at Mon Jan 11 11:47:23 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 2: Loss A: 0.686, Accuracy A: 55.120%, Loss B: 0.685, Accuracy B: 55.255%\n",
            "Epoch 2: Val_A_Loss: 0.672, Val_A_Accuracy: 62.650%, Val_B_Loss: 0.669, Val_B_Accuracy: 63.950%\n",
            "End at Mon Jan 11 11:48:06 2021\n",
            "Used 43.36 seconds\n",
            "[INFO] starting epoch 3/30...\n",
            "Start at Mon Jan 11 11:48:06 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 3: Loss A: 0.677, Accuracy A: 56.907%, Loss B: 0.675, Accuracy B: 57.823%\n",
            "Epoch 3: Val_A_Loss: 0.655, Val_A_Accuracy: 64.500%, Val_B_Loss: 0.651, Val_B_Accuracy: 67.100%\n",
            "End at Mon Jan 11 11:48:49 2021\n",
            "Used 42.9 seconds\n",
            "[INFO] starting epoch 4/30...\n",
            "Start at Mon Jan 11 11:48:49 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 4: Loss A: 0.665, Accuracy A: 59.010%, Loss B: 0.662, Accuracy B: 60.211%\n",
            "Epoch 4: Val_A_Loss: 0.638, Val_A_Accuracy: 67.900%, Val_B_Loss: 0.631, Val_B_Accuracy: 70.200%\n",
            "End at Mon Jan 11 11:49:30 2021\n",
            "Used 41.66 seconds\n",
            "[INFO] starting epoch 5/30...\n",
            "Start at Mon Jan 11 11:49:30 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 5: Loss A: 0.651, Accuracy A: 61.434%, Loss B: 0.647, Accuracy B: 61.947%\n",
            "Epoch 5: Val_A_Loss: 0.623, Val_A_Accuracy: 70.400%, Val_B_Loss: 0.614, Val_B_Accuracy: 71.950%\n",
            "End at Mon Jan 11 11:50:12 2021\n",
            "Used 41.34 seconds\n",
            "[INFO] starting epoch 6/30...\n",
            "Start at Mon Jan 11 11:50:12 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 6: Loss A: 0.636, Accuracy A: 62.747%, Loss B: 0.631, Accuracy B: 63.361%\n",
            "Epoch 6: Val_A_Loss: 0.609, Val_A_Accuracy: 71.550%, Val_B_Loss: 0.598, Val_B_Accuracy: 73.000%\n",
            "End at Mon Jan 11 11:50:53 2021\n",
            "Used 40.77 seconds\n",
            "[INFO] starting epoch 7/30...\n",
            "Start at Mon Jan 11 11:50:53 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 7: Loss A: 0.620, Accuracy A: 64.487%, Loss B: 0.614, Accuracy B: 64.962%\n",
            "Epoch 7: Val_A_Loss: 0.596, Val_A_Accuracy: 72.650%, Val_B_Loss: 0.585, Val_B_Accuracy: 74.350%\n",
            "End at Mon Jan 11 11:51:33 2021\n",
            "Used 40.43 seconds\n",
            "[INFO] starting epoch 8/30...\n",
            "Start at Mon Jan 11 11:51:33 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 8: Loss A: 0.605, Accuracy A: 65.897%, Loss B: 0.598, Accuracy B: 66.452%\n",
            "Epoch 8: Val_A_Loss: 0.584, Val_A_Accuracy: 72.700%, Val_B_Loss: 0.574, Val_B_Accuracy: 74.350%\n",
            "End at Mon Jan 11 11:52:13 2021\n",
            "Used 39.96 seconds\n",
            "[INFO] starting epoch 9/30...\n",
            "Start at Mon Jan 11 11:52:13 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 9: Loss A: 0.587, Accuracy A: 67.588%, Loss B: 0.579, Accuracy B: 67.781%\n",
            "Epoch 9: Val_A_Loss: 0.573, Val_A_Accuracy: 73.550%, Val_B_Loss: 0.564, Val_B_Accuracy: 74.050%\n",
            "End at Mon Jan 11 11:52:52 2021\n",
            "Used 39.18 seconds\n",
            "[INFO] starting epoch 10/30...\n",
            "Start at Mon Jan 11 11:52:52 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 10: Loss A: 0.568, Accuracy A: 69.323%, Loss B: 0.560, Accuracy B: 69.509%\n",
            "Epoch 10: Val_A_Loss: 0.563, Val_A_Accuracy: 73.550%, Val_B_Loss: 0.557, Val_B_Accuracy: 73.750%\n",
            "End at Mon Jan 11 11:53:31 2021\n",
            "Used 38.67 seconds\n",
            "[INFO] starting epoch 11/30...\n",
            "Start at Mon Jan 11 11:53:31 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 11: Loss A: 0.550, Accuracy A: 70.423%, Loss B: 0.540, Accuracy B: 71.357%\n",
            "Epoch 11: Val_A_Loss: 0.550, Val_A_Accuracy: 74.600%, Val_B_Loss: 0.548, Val_B_Accuracy: 73.800%\n",
            "End at Mon Jan 11 11:54:09 2021\n",
            "Used 37.92 seconds\n",
            "[INFO] starting epoch 12/30...\n",
            "Start at Mon Jan 11 11:54:09 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 12: Loss A: 0.531, Accuracy A: 72.220%, Loss B: 0.521, Accuracy B: 72.805%\n",
            "Epoch 12: Val_A_Loss: 0.543, Val_A_Accuracy: 74.350%, Val_B_Loss: 0.540, Val_B_Accuracy: 74.150%\n",
            "End at Mon Jan 11 11:54:46 2021\n",
            "Used 37.11 seconds\n",
            "[INFO] starting epoch 13/30...\n",
            "Start at Mon Jan 11 11:54:46 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 13: Loss A: 0.509, Accuracy A: 74.061%, Loss B: 0.500, Accuracy B: 74.245%\n",
            "Epoch 13: Val_A_Loss: 0.534, Val_A_Accuracy: 74.250%, Val_B_Loss: 0.530, Val_B_Accuracy: 74.600%\n",
            "End at Mon Jan 11 11:55:23 2021\n",
            "Used 36.76 seconds\n",
            "[INFO] starting epoch 14/30...\n",
            "Start at Mon Jan 11 11:55:23 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 14: Loss A: 0.488, Accuracy A: 75.544%, Loss B: 0.478, Accuracy B: 76.031%\n",
            "Epoch 14: Val_A_Loss: 0.527, Val_A_Accuracy: 74.650%, Val_B_Loss: 0.524, Val_B_Accuracy: 75.200%\n",
            "End at Mon Jan 11 11:55:59 2021\n",
            "Used 36.38 seconds\n",
            "[INFO] starting epoch 15/30...\n",
            "Start at Mon Jan 11 11:55:59 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 15: Loss A: 0.466, Accuracy A: 77.369%, Loss B: 0.454, Accuracy B: 77.935%\n",
            "Epoch 15: Val_A_Loss: 0.517, Val_A_Accuracy: 75.050%, Val_B_Loss: 0.518, Val_B_Accuracy: 75.150%\n",
            "End at Mon Jan 11 11:56:35 2021\n",
            "Used 36.09 seconds\n",
            "[INFO] starting epoch 16/30...\n",
            "Start at Mon Jan 11 11:56:35 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 16: Loss A: 0.440, Accuracy A: 79.456%, Loss B: 0.429, Accuracy B: 79.916%\n",
            "Epoch 16: Val_A_Loss: 0.511, Val_A_Accuracy: 75.500%, Val_B_Loss: 0.511, Val_B_Accuracy: 75.300%\n",
            "End at Mon Jan 11 11:57:11 2021\n",
            "Used 35.61 seconds\n",
            "[INFO] starting epoch 17/30...\n",
            "Start at Mon Jan 11 11:57:11 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 17: Loss A: 0.416, Accuracy A: 81.550%, Loss B: 0.405, Accuracy B: 81.933%\n",
            "Epoch 17: Val_A_Loss: 0.506, Val_A_Accuracy: 75.900%, Val_B_Loss: 0.505, Val_B_Accuracy: 75.500%\n",
            "End at Mon Jan 11 11:57:46 2021\n",
            "Used 34.82 seconds\n",
            "[INFO] starting epoch 18/30...\n",
            "Start at Mon Jan 11 11:57:46 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 18: Loss A: 0.392, Accuracy A: 82.786%, Loss B: 0.381, Accuracy B: 83.616%\n",
            "Epoch 18: Val_A_Loss: 0.495, Val_A_Accuracy: 77.050%, Val_B_Loss: 0.499, Val_B_Accuracy: 76.050%\n",
            "End at Mon Jan 11 11:58:19 2021\n",
            "Used 33.78 seconds\n",
            "[INFO] starting epoch 19/30...\n",
            "Start at Mon Jan 11 11:58:19 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 19: Loss A: 0.368, Accuracy A: 84.819%, Loss B: 0.355, Accuracy B: 85.070%\n",
            "Epoch 19: Val_A_Loss: 0.493, Val_A_Accuracy: 77.100%, Val_B_Loss: 0.492, Val_B_Accuracy: 76.900%\n",
            "End at Mon Jan 11 11:58:53 2021\n",
            "Used 33.48 seconds\n",
            "[INFO] starting epoch 20/30...\n",
            "Start at Mon Jan 11 11:58:53 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 20: Loss A: 0.340, Accuracy A: 87.059%, Loss B: 0.328, Accuracy B: 87.427%\n",
            "Epoch 20: Val_A_Loss: 0.495, Val_A_Accuracy: 77.050%, Val_B_Loss: 0.490, Val_B_Accuracy: 76.950%\n",
            "End at Mon Jan 11 11:59:25 2021\n",
            "Used 32.65 seconds\n",
            "[INFO] starting epoch 21/30...\n",
            "Start at Mon Jan 11 11:59:25 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 21: Loss A: 0.314, Accuracy A: 88.785%, Loss B: 0.302, Accuracy B: 89.123%\n",
            "Epoch 21: Val_A_Loss: 0.491, Val_A_Accuracy: 77.600%, Val_B_Loss: 0.491, Val_B_Accuracy: 77.000%\n",
            "End at Mon Jan 11 11:59:58 2021\n",
            "Used 32.64 seconds\n",
            "[INFO] starting epoch 22/30...\n",
            "Start at Mon Jan 11 11:59:58 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 0\n",
            "Epoch 22: Loss A: 0.284, Accuracy A: 90.892%, Loss B: 0.273, Accuracy B: 91.016%\n",
            "Epoch 22: Val_A_Loss: 0.489, Val_A_Accuracy: 78.000%, Val_B_Loss: 0.492, Val_B_Accuracy: 77.450%\n",
            "End at Mon Jan 11 12:00:30 2021\n",
            "Used 31.99 seconds\n",
            "[INFO] starting epoch 23/30...\n",
            "Start at Mon Jan 11 12:00:30 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 1\n",
            "Epoch 23: Loss A: 0.257, Accuracy A: 92.699%, Loss B: 0.247, Accuracy B: 92.806%\n",
            "Epoch 23: Val_A_Loss: 0.494, Val_A_Accuracy: 77.850%, Val_B_Loss: 0.496, Val_B_Accuracy: 77.650%\n",
            "End at Mon Jan 11 12:01:01 2021\n",
            "Used 31.14 seconds\n",
            "[INFO] starting epoch 24/30...\n",
            "Start at Mon Jan 11 12:01:01 2021\n",
            "22528/23000 [============================>.] -  metrics\n",
            "Early Stop count 2\n",
            "Epoch 24: Loss A: 0.233, Accuracy A: 94.063%, Loss B: 0.221, Accuracy B: 94.742%\n",
            "Epoch 24: Val_A_Loss: 0.500, Val_A_Accuracy: 77.750%, Val_B_Loss: 0.504, Val_B_Accuracy: 77.800%\n",
            "End at Mon Jan 11 12:01:32 2021\n",
            "Used 30.77 seconds\n",
            "Early Stopping!!\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.6895 - acc: 0.7064\n",
            "[INFO] test accuracy: 0.7857\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.6978 - acc: 0.7045\n",
            "[INFO] test accuracy: 0.7833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lmpNVpwJwHe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}